import rclpy
from rclpy.node import Node
from ultralytics import YOLO
import cv2
import threading
import numpy as np
import os
from insightface.app import FaceAnalysis
import time
import mediapipe as mp
import subprocess
import sys

class VisionMasterNode(Node):
    def __init__(self):
        super().__init__('vision_master_node')

        # ========================== 1. Load YOLOv8-Pose ==========================
        self.model = YOLO("yolov8n-pose.pt")
        self.model.fuse()

        # ========================== 2. Load InsightFace ==========================
        self.face_app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])
        self.face_app.prepare(ctx_id=0, det_size=(480, 480))

        # ========================== 3. Preload known faces ==========================
        self.known_faces = []
        self.known_faces_folder = "known_faces"
        self.load_known_faces()

        # ========================== 4. MediaPipe Hand ==========================
        os.environ["MEDIAPIPE_DISABLE_XNNPACK"] = "1"
        self.mp_hands = mp.solutions.hands
        self.mp_draw = mp.solutions.drawing_utils
        self.hands_detector = self.mp_hands.Hands(static_image_mode=False, max_num_hands=1,
                                                  min_detection_confidence=0.8, min_tracking_confidence=0.8)

        # ========================== 5. Camera ==========================
        self.cap = cv2.VideoCapture(0)
        self.cap.set(3, 1280)
        self.cap.set(4, 720)
        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

        self.frame_latest = None
        self.lock_frame = threading.Lock()
        self.running = True

        threading.Thread(target=self.camera_thread, daemon=True).start()

        # ========================== 6. Control flags ==========================
        self.last_launched_cmd = ""
        self.hold_start = None
        self.hold_start_left = None
        self.current_ros_cmd = None
        self.ros_process = None
        self.virtual_active = False

        self.launch_map = {
            0: ("my_bot", "ball_tracker.launch.py"),
            1: ("my_bot", "follow_person.launch.py"),
            2: ("my_bot", "virtual_steering.launch.py"),
            3: ("my_bot", "go_wait_point.launch.py"),
            4: ("my_bot", "return_start.launch.py"),
            5: ("my_bot", "stop.launch.py"),
        }

        # ========================== 7. Processing thread ==========================
        self.chosen_idx_last = None
        self.chosen_hold_frames = 3
        self.frames_no_face = 0

        threading.Thread(target=self.processing_thread, daemon=True).start()

    # ========================== Load known faces ==========================
    def load_known_faces(self):
        for person_name in os.listdir(self.known_faces_folder):
            person_folder = os.path.join(self.known_faces_folder, person_name)
            if not os.path.isdir(person_folder):
                continue
            embeddings = []
            for filename in os.listdir(person_folder):
                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                    img_path = os.path.join(person_folder, filename)
                    img = cv2.imread(img_path)
                    if img is None:
                        self.get_logger().warn(f"Cannot read image: {img_path}")
                        continue
                    faces = self.face_app.get(img)
                    if len(faces) > 0:
                        embeddings.append(faces[0].embedding)
            if len(embeddings) > 0:
                avg_embedding = np.mean(embeddings, axis=0)
                self.known_faces.append({"name": person_name, "embedding": avg_embedding})
                self.get_logger().info(f"Loaded {len(embeddings)} images for {person_name}")
            else:
                self.get_logger().warn(f"No face detected in folder {person_name}")

    # ========================== Camera thread ==========================
    def camera_thread(self):
        while self.running:
            ret, img = self.cap.read()
            if not ret:
                continue
            img = cv2.flip(img, 1)
            with self.lock_frame:
                self.frame_latest = img.copy()
            time.sleep(0.001)

    # ========================== Face matching ==========================
    def match_face(self, embedding, threshold=0.3):
        best_name = "Unknown"
        best_score = threshold
        for entry in self.known_faces:
            sim = np.dot(embedding, entry["embedding"]) / (np.linalg.norm(embedding) * np.linalg.norm(entry["embedding"]))
            if sim > best_score:
                best_score = sim
                best_name = entry["name"]
        return best_name

    # ========================== Angle calculation ==========================
    @staticmethod
    def calculate_angle(a, b, c):
        a, b, c = np.array(a), np.array(b), np.array(c)
        radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])
        angle = np.abs(radians * 180.0 / np.pi)
        if angle > 180.0:
            angle = 360.0 - angle
        return angle

    # ========================== ROS helpers ==========================
    def launch_ros(self, count):
        pkg, file = self.launch_map[count]
        self.get_logger().info(f"Launching ROS2: {pkg} {file}")
        self.cap.release()
        cv2.destroyAllWindows()
        self.ros_process = subprocess.Popen(["ros2", "launch", pkg, file])
        self.current_ros_cmd = count
        sys.exit(0)

    def stop_ros(self):
        if self.ros_process is not None:
            self.ros_process.terminate()
            self.ros_process = None
            self.get_logger().info(">>> ROS2 STOPPED.")
        self.current_ros_cmd = None

    def wait_and_stop_left(self, count):
        if count != self.current_ros_cmd:
            self.hold_start_left = None
            return
        if self.hold_start_left is None:
            self.hold_start_left = time.time()
            return
        if time.time() - self.hold_start_left >= 5.0:
            self.stop_ros()
            self.hold_start_left = None

    # ========================== Virtual steering ==========================
    def run_virtual_steering(self):
        from math import cos, sin, pi

        vs_hands = self.mp_hands.Hands(
            max_num_hands=2, min_detection_confidence=0.6, min_tracking_confidence=0.6
        )
        self.virtual_active = True

        # Virtual steering state
        wheel_base = 40
        speed_base = 1.0
        turn_gain = 0.03
        heading = 0.0
        pos_x, pos_y = 400, 300
        map_w, map_h = 800, 600
        car_length = 40

        drive_enabled = False
        smoothed_angle = 0.0
        alpha = 0.25
        deadzone = 5
        v_r_smooth = 0.0
        v_l_smooth = 0.0
        vel_alpha = 0.25
        fist_hold_start = None
        required_hold_time = 1.0

        two_fingers_start = None
        required_two_fingers_time = 5.0
        radius = 150

        def is_fist(hand_landmarks):
            tips = [8, 12, 16, 20]
            bases = [5, 9, 13, 17]
            count_fold = sum(
                hand_landmarks.landmark[t].y > hand_landmarks.landmark[b].y
                for t, b in zip(tips, bases)
            )
            return count_fold >= 3

        def is_two_fingers(hand_landmarks):
            tips = [8, 12]
            bases = [5, 9]
            up = sum(
                hand_landmarks.landmark[t].y < hand_landmarks.landmark[b].y
                for t, b in zip(tips, bases)
            )
            folded = (
                hand_landmarks.landmark[16].y > hand_landmarks.landmark[13].y
                and hand_landmarks.landmark[20].y > hand_landmarks.landmark[17].y
            )
            return up == 2 and folded

        try:
            while self.running:
                with self.lock_frame:
                    frame = self.frame_latest.copy() if self.frame_latest is not None else None
                if frame is None:
                    time.sleep(0.01)
                    continue

                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = vs_hands.process(frame_rgb)

                wrist_left = wrist_right = None
                left_land = right_land = None

                if results.multi_hand_landmarks and results.multi_handedness:
                    for hand_landmarks, hand_info in zip(
                        results.multi_hand_landmarks, results.multi_handedness
                    ):
                        hand_type = hand_info.classification[0].label
                        self.mp_draw.draw_landmarks(
                            frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS
                        )
                        wrist = hand_landmarks.landmark[0]
                        wx = int(wrist.x * frame.shape[1])
                        wy = int(wrist.y * frame.shape[0])
                        if hand_type == "Left":
                            wrist_left, left_land = (wx, wy), hand_landmarks
                        else:
                            wrist_right, right_land = (wx, wy), hand_landmarks

                # --- Hand behavior logic ---
                hand_count = (1 if left_land else 0) + (1 if right_land else 0)
                direction = "STOP"
                moving = False

                # Two-hand behavior
                if hand_count == 2:
                    left_fist = is_fist(left_land)
                    right_fist = is_fist(right_land)
                    if left_fist and right_fist:
                        if not drive_enabled:
                            if fist_hold_start is None:
                                fist_hold_start = time.time()
                            elif time.time() - fist_hold_start >= required_hold_time:
                                drive_enabled = True
                        direction = "Forward"
                    else:
                        drive_enabled = False
                        fist_hold_start = None
                        direction = "STOP"

                # One-hand behavior
                elif hand_count == 1:
                    hand = left_land if left_land else right_land
                    if is_fist(hand):
                        direction = "Backward"
                        drive_enabled = True
                        fist_hold_start = None
                        two_fingers_start = None
                    else:
                        if left_land is not None and is_two_fingers(left_land):
                            if two_fingers_start is None:
                                two_fingers_start = time.time()
                            elapsed = time.time() - two_fingers_start
                            remaining = max(0, required_two_fingers_time - elapsed)
                            cv2.putText(
                                frame,
                                f"Exit in: {remaining:.1f}s",
                                (50, 50),
                                cv2.FONT_HERSHEY_SIMPLEX,
                                1.0,
                                (0, 0, 255),
                                2,
                            )
                            if elapsed >= required_two_fingers_time:
                                print(
                                    "[virtual_steering] left-hand 2-fingers hold -> exit"
                                )
                                break
                            direction = "STOP"
                            drive_enabled = False
                        else:
                            direction = "STOP"
                            drive_enabled = False
                            fist_hold_start = None
                            two_fingers_start = None
                else:
                    drive_enabled = False
                    fist_hold_start = None
                    two_fingers_start = None
                    direction = "STOP"

                moving = drive_enabled and direction != "STOP"

                # --- Steering & differential drive ---
                angle = 0
                cx = cy = None
                if wrist_left and wrist_right:
                    xL, yL = wrist_left
                    xR, yR = wrist_right
                    cx = (xL + xR) // 2
                    cy = (yL + yR) // 2
                    dx = xR - xL
                    dy = yR - yL
                    raw_angle = np.clip(np.degrees(np.arctan2(dy, dx)), -90, 90)
                    smoothed_angle = alpha * raw_angle + (1 - alpha) * smoothed_angle
                    angle = 0 if abs(smoothed_angle) < deadzone else smoothed_angle

                if moving:
                    if direction == "Forward":
                        turn_rate = angle * turn_gain
                        v_r = speed_base + turn_rate
                        v_l = speed_base - turn_rate
                    elif direction == "Backward":
                        v_r = -speed_base
                        v_l = -speed_base
                    else:
                        v_r = v_l = 0

                    v_r_smooth = vel_alpha * v_r + (1 - vel_alpha) * v_r_smooth
                    v_l_smooth = vel_alpha * v_l + (1 - vel_alpha) * v_l_smooth
                    v = (v_r_smooth + v_l_smooth) / 2.0
                    omega = (v_r_smooth - v_l_smooth) / wheel_base
                    heading += omega
                    pos_x += v * cos(heading)
                    pos_y += v * sin(heading)
                    pos_x = np.clip(pos_x, 0, map_w)
                    pos_y = np.clip(pos_y, 0, map_h)

                # --- Draw UI ---
                if cx is not None:
                    cv2.circle(frame, (cx, cy), radius, (0, 255, 0), 3)
                    rad = np.deg2rad(smoothed_angle)
                    vx, vy = cos(rad), sin(rad)
                    start_point = (int(cx - vx * radius), int(cy - vy * radius))
                    end_point = (int(cx + vx * radius), int(cy + vy * radius))
                    cv2.line(frame, start_point, end_point, (255, 0, 0), 3, cv2.LINE_AA)

                canvas = np.ones((map_h, map_w, 3), dtype=np.uint8) * 255
                x_head = int(pos_x + (car_length / 2) * cos(heading))
                y_head = int(pos_y + (car_length / 2) * sin(heading))
                x_tail = int(pos_x - (car_length / 2) * cos(heading))
                y_tail = int(pos_y - (car_length / 2) * sin(heading))
                cv2.line(canvas, (x_tail, y_tail), (x_head, y_head), (0, 0, 255), 3)

                cv2.putText(
                    canvas,
                    f"Dir: {direction} Angle: {int(angle)} RUN={drive_enabled}",
                    (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.8,
                    (0, 0, 0),
                    2,
                )

                cv2.imshow("Virtual Steering", frame)
                cv2.imshow("Map", canvas)

                if cv2.waitKey(1) & 0xFF == ord("q"):
                    print("[virtual_steering] 'q' pressed -> exiting")
                    break

                time.sleep(0.002)

        finally:
            cv2.destroyAllWindows()
            self.virtual_active = False
            print("[virtual_steering] exited -> returning to main")

    # ========================== Hand processing ==========================
    def process_hand(self, frame, x1, y1, x2, y2, annotated):
        """
        Main hand processing used in the main processing loop.
        Right-hand 2-fingers held 5s -> start integrated virtual steering (blocking).
        Left-hand hold still -> stop ROS (original behavior).
        """
        # Thay global bằng self
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        hands_result = self.hands_detector.process(frame_rgb)
        left_count, right_count = -1, -1
        left_hand_cmd, right_hand_cmd = "", ""
        hand_detected = False

        # helpers
        def launch_ros_local(count):
            # call global launch_ros for non-virtual-steering
            self.launch_ros(count)

        def wait_and_stop_left_local(count):
            self.wait_and_stop_left(count)

        def wait_and_launch_local(count):
            # xử lý hold 5s cho right-hand
            if self.last_launched_cmd == f"finger_{count}":
                return

            if self.hold_start is None:
                self.hold_start = time.time()
                return

            if time.time() - self.hold_start >= 5.0:
                self.last_launched_cmd = f"finger_{count}"
                if count == 2:
                    print(">>> right-hand 2-fingers held 5s: launching integrated Virtual Steering.")
                    try:
                        cv2.destroyWindow("Robot mapping")
                    except:
                        pass
                    self.hold_start = None
                    # run virtual steering (blocking)
                    self.run_virtual_steering()
                    self.last_launched_cmd = ""
                else:
                    launch_ros_local(count)

        # ========================== PROCESS HANDS ==========================
        if hands_result.multi_hand_landmarks and hands_result.multi_handedness:
            for hand_lms, handedness in zip(hands_result.multi_hand_landmarks, hands_result.multi_handedness):

                cx = int(np.mean([lm.x for lm in hand_lms.landmark]) * frame.shape[1])
                cy = int(np.mean([lm.y for lm in hand_lms.landmark]) * frame.shape[0])

                # restrict to person bbox
                if not (x1 <= cx <= x2 and y1 <= cy <= y2):
                    continue

                label = handedness.classification[0].label

                h, w, _ = frame.shape
                pts = [(int(lm.x * w), int(lm.y * h)) for lm in hand_lms.landmark]

                tip_ids = [4, 8, 12, 16, 20]
                fingers = []

                # thumb
                if label == "Left":
                    fingers.append(1 if pts[4][0] > pts[3][0] else 0)
                else:
                    fingers.append(1 if pts[4][0] < pts[3][0] else 0)

                for tid in tip_ids[1:]:
                    fingers.append(1 if pts[tid][1] < pts[tid - 2][1] else 0)

                count = fingers.count(1)

                cmds = [
                    "Follow object",
                    "Follow person",
                    "Control by virtual steering",
                    "Go to wait point",
                    "Return to the start point",
                    "Stop",
                ]

                if label == "Left":
                    left_count = count
                    left_hand_cmd = cmds[count]
                    wait_and_stop_left_local(count)
                else:
                    right_count = count
                    right_hand_cmd = cmds[count]
                    wait_and_launch_local(count)

                self.mp_draw.draw_landmarks(annotated, hand_lms, self.mp_hands.HAND_CONNECTIONS)
                hand_detected = True
        else:
            # no hand detected
            self.hold_start = None
            self.hold_start_left = None

        return hand_detected, left_hand_cmd, right_hand_cmd, left_count, right_count


    # ========================== Arm command ==========================
    def get_arm_cmd(self, keypoints):
        class TmpLandmark:
            x = 0
            y = 0

        # Build landmarks dictionary
        landmarks = {j: TmpLandmark() for j in range(len(keypoints))}
        for j, kp in enumerate(keypoints):
            landmarks[j].x = float(kp[0])
            landmarks[j].y = float(kp[1])

        # Calculate angles
        r_sh_angle = self.calculate_angle(
            [landmarks[12].x, landmarks[12].y],
            [landmarks[6].x, landmarks[6].y],
            [landmarks[8].x, landmarks[8].y]
        )
        r_el_angle = self.calculate_angle(
            [landmarks[6].x, landmarks[6].y],
            [landmarks[8].x, landmarks[8].y],
            [landmarks[10].x, landmarks[10].y]
        )
        l_sh_angle = self.calculate_angle(
            [landmarks[11].x, landmarks[11].y],
            [landmarks[5].x, landmarks[5].y],
            [landmarks[7].x, landmarks[7].y]
        )
        l_el_angle = self.calculate_angle(
            [landmarks[5].x, landmarks[5].y],
            [landmarks[7].x, landmarks[7].y],
            [landmarks[9].x, landmarks[9].y]
        )

        # Determine arm commands
        l_arm_cmd = ''
        r_arm_cmd = ''

        if 150 <= r_sh_angle <= 180 and 150 <= r_el_angle <= 180:
            r_arm_cmd = "STOP"
        elif 70 <= r_sh_angle <= 110 and 150 <= r_el_angle <= 180:
            r_arm_cmd = "RUN"

        if 150 <= l_sh_angle <= 180 and 150 <= l_el_angle <= 180:
            l_arm_cmd = "STOP"
        elif 70 <= l_sh_angle <= 110 and 150 <= l_el_angle <= 180:
            l_arm_cmd = "RUN"

        return l_arm_cmd, r_arm_cmd, (l_sh_angle, l_el_angle, r_sh_angle, r_el_angle)

    # ========================== Main processing thread ==========================
    def processing_thread(self):
        chosen_idx_last = None
        frames_no_face = 0
        chosen_hold_frames = 3

        final_cmd = ""
        current_detected_cmd = ""
        final_cmd_timer = None
        confirm_duration = 5.0

        while self.running:
            try:
                # Pause main logic if virtual steering is active
                if self.virtual_active:
                    time.sleep(0.02)
                    continue

                if self.frame_latest is None:
                    time.sleep(0.005)
                    continue

                with self.lock_frame:
                    frame = self.frame_latest.copy()

                annotated = frame.copy()
                h_img, w_img = frame.shape[:2]

                results = self.model.predict(frame, imgsz=480, conf=0.5, iou=0.5,
                                            verbose=False, half=False, agnostic_nms=True)
                pose_bboxes = []
                keypoints_all = None

                for r in results:
                    annotated = r.plot(labels=False)
                    if r.boxes is None or len(r.boxes.xyxy) == 0:
                        continue
                    pose_bboxes = [list(map(int, box.cpu().numpy())) for box in r.boxes.xyxy]
                    keypoints_all = r.keypoints.xy

                if len(pose_bboxes) == 0:
                    cv2.imshow("Robot mapping", annotated)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        self.running = False
                    continue

                face_names_all = []
                face_areas_all = []

                for i, (x1, y1, x2, y2) in enumerate(pose_bboxes):
                    x1c = max(0, min(w_img-1, x1))
                    y1c = max(0, min(h_img-1, y1))
                    x2c = max(0, min(w_img-1, x2))
                    y2c = max(0, min(h_img-1, y2))

                    if x2c <= x1c or y2c <= y1c:
                        face_names_all.append("Unknown")
                        face_areas_all.append(0)
                        continue

                    person_crop = frame[y1c:y2c, x1c:x2c]

                    try:
                        faces = self.face_app.get(person_crop)
                    except Exception as e:
                        print("[WARN] face_app.get error:", e)
                        faces = []

                    if len(faces) > 0:
                        face = faces[0]
                        emb = face.embedding
                        name = self.match_face(emb)
                        fx1, fy1, fx2, fy2 = face.bbox.astype(int)
                        fx1 = np.clip(fx1, 0, person_crop.shape[1]-1)
                        fy1 = np.clip(fy1, 0, person_crop.shape[0]-1)
                        fx2 = np.clip(fx2, 0, person_crop.shape[1]-1)
                        fy2 = np.clip(fy2, 0, person_crop.shape[0]-1)
                        face_area = max(0, (fx2 - fx1) * (fy2 - fy1))
                    else:
                        name = "Unknown"
                        face_area = 0

                    face_names_all.append(name)
                    face_areas_all.append(face_area)
                    cv2.putText(annotated, name, (x1c, max(0, y1c-6)),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6,
                                (0,255,0) if name!="Unknown" else (0,128,255), 2)

                # Choose nearest known person by largest face area
                chosen_idx = None
                max_face_area = 0
                for i, name in enumerate(face_names_all):
                    if name != "Unknown" and face_areas_all[i] > max_face_area:
                        chosen_idx = i
                        max_face_area = face_areas_all[i]

                # SAFE-DETECT: giữ người đã chọn trong vài frame nếu mất mặt
                if chosen_idx is None:
                    if chosen_idx_last is not None:
                        frames_no_face += 1
                        if frames_no_face <= chosen_hold_frames:
                            chosen_idx = chosen_idx_last
                        else:
                            chosen_idx_last = None
                else:
                    frames_no_face = 0

                if chosen_idx is not None:
                    chosen_idx_last = chosen_idx

                # Safe keypoints access
                if keypoints_all is None or chosen_idx is None or chosen_idx >= len(keypoints_all):
                    person_kps = np.zeros((17, 2))
                else:
                    person_kps = keypoints_all[chosen_idx]

                # Process hands and arm commands
                if chosen_idx is not None:
                    x1, y1, x2, y2 = pose_bboxes[chosen_idx]
                    x1 = max(0, min(w_img-1, int(x1)))
                    y1 = max(0, min(h_img-1, int(y1)))
                    x2 = max(0, min(w_img-1, int(x2)))
                    y2 = max(0, min(h_img-1, int(y2)))

                    hand_detected, left_hand_cmd, right_hand_cmd, left_count, right_count = self.process_hand(
                        frame, x1, y1, x2, y2, annotated)

                    l_arm_cmd, r_arm_cmd, angles = self.get_arm_cmd(person_kps)
                    hand_cmd = left_hand_cmd if left_hand_cmd != "" else right_hand_cmd
                    arm_cmd = r_arm_cmd if r_arm_cmd != "" else l_arm_cmd
                    new_detected_cmd = hand_cmd if hand_detected else arm_cmd

                    if new_detected_cmd != current_detected_cmd:
                        current_detected_cmd = new_detected_cmd
                        final_cmd_timer = time.time() if new_detected_cmd != "" else None

                    if final_cmd_timer is not None:
                        elapsed = time.time() - final_cmd_timer
                        if elapsed >= confirm_duration:
                            final_cmd = current_detected_cmd
                    else:
                        elapsed = 0

                    y_offset = 25
                    if left_count != -1:
                        cv2.putText(annotated, f"L Hand: {left_count} Stop {left_hand_cmd}",
                                    (x1+5, y1+y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)
                        y_offset += 25
                    if right_count != -1:
                        cv2.putText(annotated, f"R Hand: {right_count} {right_hand_cmd}",
                                    (x1+5, y1+y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)
                        y_offset += 25
                    if l_arm_cmd != "":
                        cv2.putText(annotated, f"R Arm: {l_arm_cmd}", (x1+5, y1+y_offset),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)
                        y_offset += 25
                    if r_arm_cmd != "":
                        cv2.putText(annotated, f"L Arm: {r_arm_cmd}", (x1+5, y1+y_offset),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)
                        y_offset += 25

                    cv2.putText(annotated, f"FINAL CMD: {final_cmd}", (50,50),
                                cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)
                    if final_cmd_timer is not None and new_detected_cmd != "":
                        remaining = max(0, confirm_duration - elapsed)
                        cv2.putText(annotated, f"Confirming: {remaining:.1f}s",
                                    (50,90), cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,0,255),2)
                else:
                    cv2.putText(annotated, f"FINAL CMD: {final_cmd}", (50,50),
                                cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)

                cv2.imshow("Robot mapping", annotated)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    self.running = False
                    cv2.destroyAllWindows()
                    break

            except Exception as e:
                print("[ERROR] processing_thread exception:", e)
                time.sleep(0.05)

def main(args=None):
    rclpy.init(args=args)
    node = VisionMasterNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.running = False
    finally:
        node.running = False
        time.sleep(0.2)
        if node.cap.isOpened():
            node.cap.release()
        cv2.destroyAllWindows()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
